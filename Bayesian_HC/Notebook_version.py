#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun May  4 16:52:09 2025

@author: october
"""

### üìò Introduction to Gaussian Processes for Haircut Prediction

In this notebook, we demonstrate how to use **Gaussian Process (GP)** as a prior in Bayesian regression, and how different **likelihood functions** can be chosen to fit the nature of the target variable. Our use case is the prediction of **haircuts** in financial instruments, which are non-negative and possibly skewed.

---

### üî¢ Cell 1: Import Required Libraries
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az

np.random.seed(42)
```

---

### üìê Cell 2: Define RBF Kernel Manually (No sklearn dependency)
```python
def rbf_kernel(X1, X2, length_scale=1.0, variance=1.0):
    """
    Radial Basis Function (RBF) kernel implementation.
    """
    X1 = np.atleast_2d(X1)
    X2 = np.atleast_2d(X2)
    sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + \
             np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)
    return variance * np.exp(-0.5 / length_scale**2 * sqdist)
```

---

### üìä Cell 3: Explanation - GP Prior and Covariance Matrix
```markdown
We define the GP prior over a latent function:

\[ f(x) \sim \mathcal{GP}(0, k(x, x')) \]

Where \( k \) is the kernel function such as RBF. This defines a **distribution over functions**. The observed data \( y \) is assumed to be noisy observations of \( f(x) \).
```

---

### üìÅ Cell 4: Load and Preprocess Haircut Dataset
```python
# Load your haircut dataset
full_data = pd.read_csv("dataset.csv")
full_data = full_data[["Asset class_New Classes", "Final HC", "ModifiedDuration", "RatingRank"]].dropna()
full_data["Interaction"] = full_data["ModifiedDuration"] * full_data["RatingRank"]

# Filter CLO class
CLO = full_data[full_data["Asset class_New Classes"] == "CLO"].copy()
```

---

### üß™ Cell 5: Feature Scaling (Standardization)
```python
def standardize_column(arr):
    return (arr - np.mean(arr)) / np.std(arr)

feature_cols = ["ModifiedDuration", "RatingRank", "Interaction"]
scale_dict = {"ModifiedDuration": 3.0, "RatingRank": 1.0, "Interaction": 2.0}

X_std = []
for col in feature_cols:
    col_std = standardize_column(CLO[col].values) * scale_dict[col]
    X_std.append(col_std)

X = np.column_stack(X_std)
y = CLO["Final HC"].values
```

---

### üìö Cell 6: Explanation - GP Posterior and Likelihood Choice
```markdown
The GP defines a prior over latent functions \( f(x) \), not the observations \( y \) directly. The likelihood \( p(y | f(x)) \) defines how noisy observations are modeled.

Common choices:
- **Normal**: symmetric noise
- **StudentT**: heavy-tailed (robust to outliers)
- **Gamma / LogNormal**: for non-negative and skewed targets like Haircut

Bayes Rule:

\[ p(f | y, X) \propto p(y | f) \cdot p(f) \]
```

---

### üß† Cell 7: Build GP Model in PyMC with Chosen Likelihood
```python
with pm.Model() as gp_model:
    length_scale = pm.Gamma("length_scale", alpha=2, beta=1)
    cov_func = pm.gp.cov.ExpQuad(input_dim=X.shape[1], ls=length_scale)

    gp = pm.gp.Marginal(cov_func=cov_func)

    sigma = pm.HalfNormal("sigma", sigma=1)

    # GP prior on latent function f(x), likelihood on y
    y_obs = gp.marginal_likelihood("y_obs", X=X, y=y, noise=sigma)

    trace_gp = pm.sample(1000, tune=1000, return_inferencedata=True, target_accept=0.9)
```

---

### üîç Cell 8: Posterior Analysis
```python
az.plot_forest(trace_gp, combined=True)
plt.title("Posterior over Hyperparameters")
plt.show()
```

---

### üìà Cell 9: Prediction from GP Model
```python
# Choose a new point to predict haircut
x_new = np.array([[3.0, 15.0, 3.0 * 15.0]])

# Apply same scaling
x_new_scaled = []
for i, col in enumerate(feature_cols):
    mean = CLO[col].mean()
    std = CLO[col].std()
    scaled = (x_new[0, i] - mean) / std * scale_dict[col]
    x_new_scaled.append(scaled)

x_new_scaled = np.array(x_new_scaled).reshape(1, -1)

with gp_model:
    mu, var = gp.predict(x_new_scaled, point=trace_gp.posterior.mean().to_dict(), diag=True, pred_noise=True)

plt.title("Predictive Posterior of Haircut")
plt.hist(np.random.normal(mu, np.sqrt(var), size=1000), bins=40, density=True)
plt.xlabel("Predicted Haircut")
plt.ylabel("Density")
plt.grid(True)
plt.show()
```

---

### ‚úÖ Conclusion
```markdown
We used Gaussian Process Regression with different likelihoods suitable for non-negative, skewed outputs like haircut. GP provided a flexible non-parametric prior, and posterior sampling gave uncertainty-aware predictions.

‚úîÔ∏è No sklearn required
‚úîÔ∏è Likelihoods chosen to match data shape
‚úîÔ∏è End-to-end Bayesian pipeline
```
